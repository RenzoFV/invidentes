# üì• Gu√≠a de Instalaci√≥n de OLLAMA

OLLAMA es necesario para generar descripciones naturales en espa√±ol. Sin embargo, el sistema puede funcionar sin √©l usando descripciones b√°sicas.

## üöÄ Instalaci√≥n R√°pida

### Windows

1. **Descargar OLLAMA:**
   - Ve a [https://ollama.ai/download](https://ollama.ai/download)
   - Descarga el instalador para Windows
   - Ejecuta el archivo `.exe` descargado

2. **Verificar instalaci√≥n:**
   ```bash
   ollama --version
   ```

3. **Iniciar OLLAMA:**
   ```bash
   ollama serve
   ```
   Deja esta terminal abierta mientras usas la aplicaci√≥n.

4. **Descargar modelo (en otra terminal):**
   ```bash
   ollama pull llama3
   ```
   O para mejor soporte en espa√±ol:
   ```bash
   ollama pull llama3.2
   ```

### Linux

```bash
# Instalar OLLAMA
curl -fsSL https://ollama.ai/install.sh | sh

# Iniciar OLLAMA
ollama serve

# Descargar modelo (en otra terminal)
ollama pull llama3
```

### macOS

```bash
# Instalar con Homebrew
brew install ollama

# O descargar desde https://ollama.ai/download

# Iniciar OLLAMA
ollama serve

# Descargar modelo (en otra terminal)
ollama pull llama3
```

## ‚úÖ Verificar que Funciona

Ejecuta el script de diagn√≥stico:

```bash
python check_ollama.py
```

Este script verificar√°:
- ‚úÖ Si OLLAMA est√° instalado
- ‚úÖ Si OLLAMA est√° corriendo
- ‚úÖ Si el modelo est√° disponible

## üîß Soluci√≥n de Problemas

### "OLLAMA no est√° en el PATH"

**Windows:**
- Reinicia la terminal despu√©s de instalar
- O agrega OLLAMA al PATH manualmente

**Linux/macOS:**
- Reinicia la terminal
- O ejecuta: `export PATH=$PATH:/usr/local/bin`

### "No se pudo conectar a OLLAMA"

1. Verifica que OLLAMA est√© corriendo:
   ```bash
   ollama serve
   ```

2. Verifica que est√© en el puerto correcto:
   - Por defecto: `http://localhost:11434`
   - Si cambias el puerto, actualiza `.env`:
     ```env
     OLLAMA_BASE_URL=http://localhost:TU_PUERTO
     ```

### "Modelo no encontrado"

Descarga el modelo:
```bash
ollama pull llama3
```

Para ver modelos disponibles:
```bash
ollama list
```

## üí° Modo sin OLLAMA

Si no puedes instalar OLLAMA, el sistema funcionar√° con descripciones b√°sicas:
- ‚úÖ Detecci√≥n de objetos funcionar√°
- ‚úÖ Descripciones simples funcionar√°n
- ‚ö†Ô∏è Descripciones naturales y contextuales NO estar√°n disponibles

El sistema mostrar√°: "‚ö†Ô∏è OLLAMA no disponible. Se usar√° modo simple."

## üìù Notas

- OLLAMA debe estar corriendo mientras usas la aplicaci√≥n
- El primer uso puede ser lento mientras descarga el modelo
- Los modelos ocupan espacio (llama3 ~4.7GB)
- Puedes usar modelos m√°s peque√±os si tienes poco espacio:
  ```bash
  ollama pull llama3.2:1b  # Versi√≥n m√°s peque√±a
  ```

